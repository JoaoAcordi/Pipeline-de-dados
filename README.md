# ğŸ¬ Pipeline de Dados com Databricks e Delta Lake â€” Arquitetura MedalhÃ£o

## ğŸ§© VisÃ£o Geral

Este projeto implementa um **Pipeline de Dados** completo utilizando o **Databricks**, **Delta Lake** e a **Arquitetura MedalhÃ£o (Medallion Architecture)**, com o objetivo de processar e analisar dados de filmes.

O processo percorre trÃªs camadas principais â€” **Landing**, **Bronze**, **Silver** e **Gold** â€” cada uma com um propÃ³sito distinto na transformaÃ§Ã£o e qualidade dos dados.

---

## ğŸ—‚ï¸ Estrutura do Projeto

```
ğŸ“ Pipeline-de-dados/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ landing/            # Dados brutos (arquivos CSV)
â”‚   â”œâ”€â”€ bronze/             # Dados limpos e padronizados
â”‚   â”œâ”€â”€ silver/             # Dados integrados
â”‚   â””â”€â”€ gold/               # Dados prontos para anÃ¡lise
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 1_landing_to_bronze.py
â”‚   â”œâ”€â”€ 2_bronze_to_silver.py
â”‚   â””â”€â”€ 3_silver_to_gold.py
â”‚
â”œâ”€â”€ docs/                   # DocumentaÃ§Ã£o do MKDocs
â”‚   â”œâ”€â”€ index.md
â”‚   â”œâ”€â”€ arquitetura.md
â”‚   â”œâ”€â”€ pipeline.md
â”‚   â”œâ”€â”€ databricks_job.md
â”‚   â””â”€â”€ contato.md
â”‚
â”œâ”€â”€ mkdocs.yml              # ConfiguraÃ§Ã£o do site MKDocs
â””â”€â”€ README.md
```

---

## âš™ï¸ Tecnologias Utilizadas

* **Databricks**
* **Apache Spark (PySpark)**
* **Delta Lake**
* **MKDocs**
* **GitHub (integraÃ§Ã£o de versionamento)**

---

## ğŸ” Fluxo da Arquitetura MedalhÃ£o

| Camada      | DescriÃ§Ã£o                                                        |
| ----------- | ---------------------------------------------------------------- |
| **Landing** | Ãrea onde os arquivos CSV originais sÃ£o armazenados.             |
| **Bronze**  | Dados brutos sÃ£o lidos, padronizados e salvos em formato Delta.  |
| **Silver**  | Dados integrados de vÃ¡rias tabelas, com tratamento e junÃ§Ãµes.    |
| **Gold**    | Dados agregados e refinados, prontos para dashboards e anÃ¡lises. |

---

## ğŸ§  Notebooks e FunÃ§Ãµes

### 1ï¸âƒ£ `1_landing_to_bronze.py`

* LÃª arquivos CSV da pasta *landing*.
* Padroniza nomes das colunas.
* Grava as tabelas em formato Delta na camada Bronze.

### 2ï¸âƒ£ `2_bronze_to_silver.py`

* LÃª tabelas Delta da camada Bronze.
* Realiza joins e normalizaÃ§Ã£o de tipos.
* Grava os dados integrados na camada Silver.

### 3ï¸âƒ£ `3_silver_to_gold.py`

* Gera mÃ©tricas finais (ex: top 10 filmes por ano).
* Escreve os resultados na camada Gold.

---

## ğŸ§± ExecuÃ§Ã£o da Pipeline no Databricks

1. **Envie os arquivos CSV** para o diretÃ³rio `/Volumes/movies_catalog/landing/`.
2. **Execute os notebooks** na ordem:

   * `1_landing_to_bronze`
   * `2_bronze_to_silver`
   * `3_silver_to_gold`
3. **Crie um Job e Pipeline** no Databricks:

   * Adicione os trÃªs notebooks.
   * Configure a execuÃ§Ã£o orquestrada.
   * Execute e valide os dados nas camadas.

---

## ğŸŒ DocumentaÃ§Ã£o com MKDocs

### 1ï¸âƒ£ Instalar o MKDocs

```bash
pip install mkdocs mkdocs-material
```

### 2ï¸âƒ£ Rodar localmente

```bash
mkdocs serve
```

O site ficarÃ¡ disponÃ­vel em:
ğŸ‘‰ [http://127.0.0.1:8000](http://127.0.0.1:8000)

### 3ï¸âƒ£ Gerar a documentaÃ§Ã£o estÃ¡tica

```bash
mkdocs build
```

Os arquivos serÃ£o gerados na pasta `site/`.

---

## ğŸ“¬ Contato

**Autor:** JoÃ£o
**InstituiÃ§Ã£o:** SATC
**Disciplina:** Engenharia de Dados
**Professor:** â€”
**Ano:** 2025

---

ğŸ¯ **Objetivo:** demonstrar um pipeline de dados funcional, documentado e orquestrado via Databricks, com boas prÃ¡ticas de organizaÃ§Ã£o e versionamento.
