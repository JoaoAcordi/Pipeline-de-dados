{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bbf723",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # 1. Camada Bronze: Ingestão dos Dados de Filmes\n",
    "# MAGIC \n",
    "# MAGIC **Objetivo:** Ler o arquivo CSV bruto da Landing Zone e salvá-lo como uma tabela Delta na camada Bronze, adicionando metadados de controle para rastreabilidade.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Importa as funções necessárias do PySpark\n",
    "from pyspark.sql.functions import current_timestamp, input_file_name\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Define o catálogo que será utilizado durante a execução\n",
    "spark.sql(\"USE CATALOG movies_catalog\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### Definindo os Caminhos de Origem e Destino\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# O caminho completo para o arquivo CSV dentro do Volume na camada de Landing\n",
    "source_file_path = \"/Volumes/movies_catalog/landing/landing_zone_movies/top_100_movies_full_best_effort.csv\"\n",
    "\n",
    "# Nome da tabela de destino na camada Bronze\n",
    "bronze_table_name = \"bronze.movies_raw\"\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### Leitura, Transformação e Carga\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Tenta executar o processo de leitura e gravação\n",
    "try:\n",
    "    # Passo 1: Leitura do arquivo CSV bruto da Landing Zone\n",
    "    # inferSchema deduz os tipos de dados das colunas\n",
    "    # header=true indica que a primeira linha do arquivo é o cabeçalho\n",
    "    df_raw = spark.read.format(\"csv\") \\\n",
    "                       .option(\"header\", \"true\") \\\n",
    "                       .option(\"inferSchema\", \"true\") \\\n",
    "                       .load(source_file_path)\n",
    "\n",
    "    # Passo 2: Adição de metadados de controle\n",
    "    #   - data_ingestao: registra quando o dado foi processado\n",
    "    #   - arquivo_origem: registra de qual arquivo o dado veio\n",
    "    df_bronze = df_raw.withColumn(\"data_ingestao\", current_timestamp()) \\\n",
    "                      .withColumn(\"arquivo_origem\", input_file_name())\n",
    "\n",
    "    # Passo 3: Salva o DataFrame como uma tabela Delta na camada Bronze\n",
    "    #   - mode(\"overwrite\"): apaga os dados existentes e os substitui pelos novos\n",
    "    #   - option(\"overwriteSchema\", \"true\"): permite que a estrutura da tabela seja atualizada se houver mudanças\n",
    "    df_bronze.write.format(\"delta\") \\\n",
    "             .mode(\"overwrite\") \\\n",
    "             .option(\"overwriteSchema\", \"true\") \\\n",
    "             .saveAsTable(bronze_table_name)\n",
    "    \n",
    "    print(f\"SUCESSO: A tabela '{bronze_table_name}' foi criada/atualizada na camada Bronze.\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Informa o erro caso o processo falhe\n",
    "    print(f\"ERRO: Não foi possível processar o arquivo. Verifique o caminho e a existência do arquivo.\")\n",
    "    print(f\"Detalhes do erro: {e}\")\n",
    "    # Levanta o erro para que o Job do Databricks registre a falha\n",
    "    raise e\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### Verificação dos Dados na Camada Bronze\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Exibe as 10 primeiras linhas da tabela recém-criada para verificação\n",
    "display(spark.table(bronze_table_name).limit(10))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
